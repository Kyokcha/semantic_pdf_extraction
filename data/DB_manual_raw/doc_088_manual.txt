Discontinuous Piecewise Polynomial Neural Networks  
[TABLE_START]
number of points, 2, 3, 4, 5, 6, 7, 8, 9, 10  
pn,max, 1, 1.25, 1.667, 1.799, 1.989, 2.083, 2.203, 2.275, 2.362  
[TABLE_END]
Table 2: Maximum overshoot fraction, pn,max, given the number of points in the polynomial interpolation (to 4 digits rounded up).  
2.3 Weight Initialization  
Weight initialization is a huge concern for neural networks (Yam and Chow (2000)).  
In this paper, the weights within a link are initialized such that F(x) (Equation ) is a line across the link, the equation is given as  
ωi = ((xi − rmin) / (rmax − rmin)) ωa + ωa , (3)  
where ωa is chosen with a uniform random distribution in the range [−1, 1].  
2.4 Choosing Ranges rmin and rmax  
Choosing proper ranges for the links is key to getting good solutions.  
The weights used in the Lagrange polynomial interpolation do not mark the limits of the polynomial value except in the linear case.  
Figure 8 shows 5 Lagrange polynomials with maximum overshoot for weights within the desired range - note that only in the linear case is the range limited by the values of the weights.  
Instead, if the weights are in the range [−ωmax, ωmax] then a given choice of weights will produce a maximum overshoot pn,max ωmax where values of pn,max are given in Table 2.  
For a given input, the maximum possible output would be pn,max ωmax which could then be the input to the next link.  
The input range for each link should be set to [−pn,max ωmax, pn,max ωmax] to account for these overshoots.  
One potential consequence of this choice of range is input value decay.  
Consider a deep network with only one unit per layer and one link between units.  
Each layer is initialized using Equation () with ωa = 1 and input range [−pn,max, pn,max].  
Suppose the input value is xin then as the input value passes through each layer it will be compressed if rmax = pn,max > ωmax by the ratio ωmax / rmax.  
After passing through n layers, the output signal will be (ωmax / rmax)^n xin.  
As a consequence, the output from each layer is pushed towards the value 0 which means fewer and fewer of the network sub links are used.  
Fortunately, two things can occur to help the situation: (1) if a discontinuity is at the origin rapid learning can still occur since if a signal is either side of 0 it will adjust disconnected weights; (2) as the network is trained, more and more of the sub links are used.  
It would seem randomly initializing the weights could alleviate this problem, however, we’ve found that symmetric initialization as in Equation () works better.  
In this paper we use rmax = −rmin = pn,max where pn,max is provided in Table 2 which gives the maximum overshoot for the polynomials when the weights are constrained to be between 1 and -1.
